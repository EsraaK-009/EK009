{
  
    
        "post0": {
            "title": "Arabic Topic Modeling with Textacy And Spacy-udpipe",
            "content": "%%capture !pip install spacy-udpipe !pip install textacy . Downloading udpipe model . First, we have to install the Arabic model from this link:Models And upload it to our colab notebook. . import spacy_udpipe import textacy import textacy.tm import pandas as pd . /usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can&#39;t initialize NVML warnings.warn(&#34;Can&#39;t initialize NVML&#34;) . . Already downloaded a model for the &#39;ar&#39; language . nlp = spacy_udpipe.load_from_path(lang=&quot;ar&quot;, path=&quot;./arabic-padt-ud-2.5-191206.udpipe&quot;, meta={&quot;description&quot;: &quot;Custom &#39;ar&#39; model&quot;}) text = &quot;القاهرة هي المكان المفضل لدي&quot; doc = nlp(text) for token in doc: print(token.text, token.lemma_, token.pos_, token.dep_) . القاهرة قَاهِرَة NOUN nsubj هي هُوَ PRON nmod المكان مَكَان NOUN ROOT المفضل المفضل ADJ amod لدي لَدَى ADP case ي هُوَ PRON nmod . Now we have our model as &quot;nlp&quot; and we can use it with many other libraries. . df = pd.read_csv(&quot;/content/Without_namesAndSW.csv&quot;) #df.info() . Topic Modeling . To get the topics we need to go through these steps: . To make it easy and use the options we have in textacy we&#39;ll convert our data to textacy&#39;s corpus. | Get tokens of every document. | Specify the vectorizer we want. | Make the doc-term-matrix. Note: This matrix can be used with gensim models if you Transpose it. | . corpus = textacy.Corpus(nlp, data=df[&#39;No_stopWords&#39;]) . print(corpus) . Corpus(29 docs, 44526 tokens) . tokenized_docs = ( (term.lemma_ for term in textacy.extract.terms(doc, ngs=1, ents=True)) for doc in corpus) . vectorizer = textacy.representations.vectorizers.Vectorizer( tf_type=&quot;linear&quot;, idf_type=&quot;smooth&quot;, norm=&quot;l2&quot;, min_df=3, max_df=0.95) . Another Note: You can get the id2word dictionary also from the vectorizer here and use it with your code. . id2word = vectorizer.id_to_term . . doc_term_matrix = vectorizer.fit_transform(tokenized_docs) . doc_term_matrix . &lt;29x1185 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 9628 stored elements in Compressed Sparse Row format&gt; . model = textacy.tm.topic_model.TopicModel(&quot;nmf&quot;, n_topics=4) model.fit(doc_term_matrix) . /usr/local/lib/python3.8/dist-packages/sklearn/decomposition/_nmf.py:1422: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead warnings.warn( /usr/local/lib/python3.8/dist-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The &#39;init&#39; value, when &#39;init=None&#39; and n_components is less than n_samples and n_features, will be changed from &#39;nndsvd&#39; to &#39;nndsvda&#39; in 1.1 (renaming of 0.26). warnings.warn( . model . TopicModel(n_topics=4, model=NMF) . Model Inspection: . Top Topic Terms: . doc_topic_matrix = model.transform(doc_term_matrix) for topic_idx, top_terms in model.top_topic_terms(vectorizer.id_to_term, topics=[0,1,2,3]): print(&quot;topic&quot;, topic_idx, &quot;:&quot;, &quot; &quot;.join(top_terms)) . topic 0 : بَنك حِسَاب المدعى اِعتِمَاد مَبلَغ فَائِدَة شَرِكَة تَارِيخ خَبِير مَديُونِيَّة topic 1 : أُجرَة ـ وَفَاء مَستاجَر بالاجرة تَكلِيف تَكرَار إِعلَان اِستِئنَاف إِخلَاء topic 2 : شَرِكَة أَوَّل قَرَار جَمعِيَّة شَرِيك تَصفِيَة عَمَل إِدَارَة ثَانِي 87 topic 3 : اَلَّذِي 2002 تَابَع يكفى دَفع قَول تَحقِيق قِسم اِثنَان وكفايت . /usr/local/lib/python3.8/dist-packages/sklearn/decomposition/_nmf.py:1422: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead warnings.warn( . Topic Weights: . for i, val in enumerate(model.topic_weights(doc_topic_matrix)): print(i, val) . 0 0.33786064340985056 1 0.4085472464974949 2 0.13763662553716402 3 0.11595548455549054 . Documents Topics: . for doc_idx, topics in model.top_doc_topics(doc_topic_matrix): print(&quot;Doc ID: &quot;, doc_idx,&quot;:&quot;, topics) . ID: 0 : (0, 3, 2) ID: 1 : (0, 2, 3) ID: 2 : (0, 2, 3) ID: 3 : (0, 3, 2) ID: 4 : (0, 3, 2) ID: 5 : (0, 3, 2) ID: 6 : (0, 3, 2) ID: 7 : (0, 3, 2) ID: 8 : (0, 3, 2) ID: 9 : (0, 2, 3) ID: 10 : (3, 2, 0) ID: 11 : (0, 2, 3) ID: 12 : (0, 2, 3) ID: 13 : (3, 2, 0) ID: 14 : (0, 3, 2) ID: 15 : (0, 1, 3) ID: 16 : (0, 1, 3) ID: 17 : (0, 1, 3) ID: 18 : (0, 1, 3) ID: 19 : (0, 1, 2) ID: 20 : (0, 1, 3) ID: 21 : (0, 2, 1) ID: 22 : (0, 1, 3) ID: 23 : (0, 1, 3) ID: 24 : (0, 1, 3) ID: 25 : (0, 1, 3) ID: 26 : (0, 1, 2) ID: 27 : (0, 2, 1) ID: 28 : (0, 2, 1) . # topics=-1, n_terms=25, sort_terms_by=&quot;seriation&quot;) . model.save(&quot;nmf-4topics.pkl&quot;) .",
            "url": "https://esraak-009.github.io/EK009/fastpages/jupyter/2023/01/26/Arabic-TopicModeling-With-Textacy.html",
            "relUrl": "/fastpages/jupyter/2023/01/26/Arabic-TopicModeling-With-Textacy.html",
            "date": " • Jan 26, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "Positional Encoder",
            "content": "Positional Encoder . مكان الكلمة في الجملة من الحاجات المهمة جدًا لينا عشان نفهم معناها و كمان بتأثر على القواعد فعشان كده محتاجين الموديل يحافظ عليها و يحطها في حساباته و هو بيترجم أو بيعمل أي عملية تانية. طبعًا الـRNN كانت بتعمل كده بالفعل و بتحافظ على الـsequence، بس احنا اتخلينا عن الجزء ده لأنه كان عامل مشكلة بس مكان الكلمة لسه مهم، فمن هنا كان لازم نلاقي بديل و هو الـPositional encoder. . بداية، نفكر سوا أسهل حاجة ممكن نفكر فيها إن كل كلمة تكون برقم من أول 1 لحد أخر الجملة، لو الجملة 7 كلمات، أخر كلمة تأخد رقم 7 بكل بساطة، بس الأبسط بيحطنا في مشاكل كتير، ماذا لو دخل للموديل بتاعنا جملة أطول من الجمل اللي اتدرب عليها؟ ماذا لو الأرقام كانت كبيرة أوي و عملت لي مشاكل في الحسابات؟ لأ و هنا كمان أنا عندي مشكلة تانية بتسببها الأرقام الكبيرة و هي إنها بتبعد الكلمات عن بعضها جدًا في الـspace! فبيقلل الـposition similarity ما بينهم و بيضرب الدنيا . فهنا بقى هنحط شوية قواعد عشان نقدر نقول إن ده encoding لمكان الكلمة و مناسب أقدر أدخله على حسابات الموديل من غير ما يبوظها: 1 - لازم يكون unique لكل كلمة. 2 -الموديل بتاعي يعرف يتعامل مع طول جملة هو حتى متعرضش له قبل كده. 3 - لازم يكون deterministic. . الحل اللي قدمته Attention is all you need بسيط جدًا و في نفس الوقت مبهر، باستخدام الـwave frequency بيعملوا encoding خاص لكل مكان في الجملة و فكرته ذكية جدًا عشان بيحقق كل الحاجات المطلوبة . . . ليه الـsin و الـcos? . احنا عاوزين حاجة تدينا رقم unique و في نفس الوقت مش كبير و له حدود، الـsin و الـcos بيكون الrange من 1 لـ -1 و كمان فكرة الترددات بتخليها تمسك أقل أقل تغير في المكان. كأنها زرار بيعلي الصوت بس مش discrete لأ، شبه الراديو القديم كل ما تعلي سِنة صغيرة بيتغير. . بس فيه مشكلة دلوقتي! احنا قولنا إن الـsin موجة! و ده معناه إنها بتدي نفس القيمة تاني كل شوية، طب ما كده مبقتش unique! عشان يتغلبوا على النقطة دي بيكون الـfrequency قليل جدًا جدًا كل ما عدد الـindex بيزيد و عشان كده المعادلة في الصورة دي بتعمل geometric progression. دي حاجة تانية بس باختصار كمثال عليها إننا بنمشي بتتابع بيقل مع كل خطوة. مثال: [ 1 , 1/2 , 1/4 , 1/8 , ...] . . في الصورة دي شكل لأربعة dim. بس، و لكن الـمتجه في العادي بيبقى أكبر من كده بكتير، فالصورة بتكون أعقد بكتير و فعلًا unique لكل position و بتبقى مترتبة في صورة matrix بحيث يقدر يلقط التغيرات . . ليه بنقسم على 10000؟ . . في حاجة لطيفة تانية، في البيبر نفسها في السطر الموجود فيه المعادلة بتقول إنهم اختاروا المعادلة دي لسبب تاني إنها هينفع مع الـrelative position encoding بسبب اثبات ما هحطه اللينك، بس ده مهم ليه؟ عشان البيبر مكانتش معمولة كهدف للـtext بس، احنا لسه عندنا graphs و أنواع داتا مختلفة كتير. فهما اختاروها عشان كده كمان. . . . الكود باستخدام numpy: . def positional_encoding(pos,model_size): POS = np.zeros((1,model_size)) for i in range(model_size): if i % 2 == 0: #even number POS[:,i] = np.sin(pos/10000 *(i / model_size)) else: POS[:,i] = np.cos(pos/10000 *( (i-1) / model_size)) return POS . . مصادر: . https://kazemnejad.com/blog/transformer_architecture_positional_encoding/ | https://www.youtube.com/watch?v=1biZfFLPRSY | https://datascience.stackexchange.com/questions/82451/why-is-10000-used-as-the-denominator-in-positional-encodings-in-the-transformer | https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model | https://timodenk.com/blog/linear-relationships-in-the-transformers-positional-encoding/ | https://github.com/tensorflow/tensor2tensor/issues/1591 | https://medium.com/swlh/elegant-intuitions-behind-positional-encodings-dc48b4a4a5d1 | .",
            "url": "https://esraak-009.github.io/EK009/transformer/2021/11/24/Positional-Encoder.html",
            "relUrl": "/transformer/2021/11/24/Positional-Encoder.html",
            "date": " • Nov 24, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello! My name is Esraa Khaled. I’m interested in NLP and Bioinformatics. .",
          "url": "https://esraak-009.github.io/EK009/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://esraak-009.github.io/EK009/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}